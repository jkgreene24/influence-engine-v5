/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: do not edit it. Instead, edit the BAML
// files and re-generate this code.
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code
import type { BamlRuntime, BamlCtxManager, ClientRegistry, Image, Audio, Collector } from "@boundaryml/baml"
import { toBamlError } from "@boundaryml/baml"
import type { Checked, Check } from "./types"
import type { partial_types } from "./partial_types"
import type * as types from "./types"
import type {Message, ResponseChat, SwitchInfluenceStyle, UserMetadata} from "./types"
import type TypeBuilder from "./type_builder"

export class LlmResponseParser {
  constructor(private runtime: BamlRuntime, private ctxManager: BamlCtxManager) {}

  
  Betty(
      llmResponse: string,
      __baml_options__?: { tb?: TypeBuilder, clientRegistry?: ClientRegistry }
  ): ResponseChat {
    try {
      const env = __baml_options__?.env ? { ...process.env, ...__baml_options__.env } : { ...process.env };
      return this.runtime.parseLlmResponse(
        "Betty",
        llmResponse,
        false,
        this.ctxManager.cloneContext(),
        __baml_options__?.tb?.__tb(),
        __baml_options__?.clientRegistry,
        env,
      ) as ResponseChat
    } catch (error) {
      throw toBamlError(error);
    }
  }
  
  InitialMessageChat(
      llmResponse: string,
      __baml_options__?: { tb?: TypeBuilder, clientRegistry?: ClientRegistry }
  ): string {
    try {
      const env = __baml_options__?.env ? { ...process.env, ...__baml_options__.env } : { ...process.env };
      return this.runtime.parseLlmResponse(
        "InitialMessageChat",
        llmResponse,
        false,
        this.ctxManager.cloneContext(),
        __baml_options__?.tb?.__tb(),
        __baml_options__?.clientRegistry,
        env,
      ) as string
    } catch (error) {
      throw toBamlError(error);
    }
  }
  
}

export class LlmStreamParser {
  constructor(private runtime: BamlRuntime, private ctxManager: BamlCtxManager) {}

  
  Betty(
      llmResponse: string,
      __baml_options__?: { tb?: TypeBuilder, clientRegistry?: ClientRegistry }
  ): partial_types.ResponseChat {
    try {
      const env = __baml_options__?.env ? { ...process.env, ...__baml_options__.env } : { ...process.env };
      return this.runtime.parseLlmResponse(
        "Betty",
        llmResponse,
        true,
        this.ctxManager.cloneContext(),
        __baml_options__?.tb?.__tb(),
        __baml_options__?.clientRegistry,
        env,
      ) as partial_types.ResponseChat
    } catch (error) {
      throw toBamlError(error);
    }
  }
  
  InitialMessageChat(
      llmResponse: string,
      __baml_options__?: { tb?: TypeBuilder, clientRegistry?: ClientRegistry }
  ): string {
    try {
      const env = __baml_options__?.env ? { ...process.env, ...__baml_options__.env } : { ...process.env };
      return this.runtime.parseLlmResponse(
        "InitialMessageChat",
        llmResponse,
        true,
        this.ctxManager.cloneContext(),
        __baml_options__?.tb?.__tb(),
        __baml_options__?.clientRegistry,
        env,
      ) as string
    } catch (error) {
      throw toBamlError(error);
    }
  }
  
}