/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: do not edit it. Instead, edit the BAML
// files and re-generate this code.
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code
const fileMap = {
  
  "betty.baml": "template_string BettyCorePrompt #\"\r\n    You are Betty — a high-touch, real-time coaching assistant designed to help users communicate with clarity, confidence, and alignment to their natural style.\r\n    Your purpose is to support users in crafting messages, making decisions, and leading conversations in a way that feels like the best version of themselves — not like someone else.\r\n\"#\r\n\r\ntemplate_string RelevantFeedbacks(relevant_feedbacks: string) #\"\r\n    {% if relevant_feedbacks %}\r\n        Here are some relevant feedbacks about similar conversation histories between Betty and the user:\r\n        {{ relevant_feedbacks }}\r\n        You can refer to the feedbacks to better help the user.\r\n    {% else %}\r\n        \"\"\r\n    {% endif %}\r\n\"#\r\n\r\ntemplate_string UserInfluenceStyle(user_influence_style: string) #\"\r\n    {% if user_influence_style %}\r\n        You are coaching a user whose influence style is {{ user_influence_style }}.\r\n    {% else %}\r\n        You are coaching a user who is not specified. Assume the user is a Catalyst.\r\n    {% endif %}\r\n\"#\r\n\r\ntemplate_string UserMemory(user_memory: string) #\"\r\n    {% if user_memory %}\r\n        Here is the user's memory:\r\n        {{ user_memory }}\r\n    {% else %}\r\n        \"\"\r\n    {% endif %}\r\n\"#\r\n\r\nclass Message {\r\n    role \"user\" | \"assistant\" @description(#\"\r\n        The role of the message.\r\n    \"#)\r\n    content string @description(#\"\r\n        The content of the message.\r\n    \"#)\r\n}\r\n\r\ntemplate_string ConversationHistory(messages: Message[]) #\"\r\n    {% if messages %}\r\n        Here is the recent conversation history between Betty and the user:\r\n        {% for message in messages %}\r\n            {{ _.role(message.role) }}\r\n            {{ message.content }}\r\n        {% endfor %}\r\n    {% else %}\r\n        \"\"\r\n    {% endif %}\r\n\"#\r\n\r\ntemplate_string UserMetadataTemplate(user_metadata: UserMetadata) #\"\r\n    {% if user_metadata %}\r\n        Here is the user's metadata:\r\n        Name: {{ user_metadata.name }}\r\n        Influence Style: {{ user_metadata.influence_style }}\r\n    {% else %}\r\n        \"\"\r\n    {% endif %}\r\n\"#\r\n\r\nclass UserMetadata {\r\n  name string @description(#\"\r\n    The name of the user.\r\n  \"#)\r\n  influence_style string @description(#\"\r\n    The influence style of the user.\r\n  \"#)\r\n}\r\n\r\nclass ResponseChat {\r\n  answer string @stream.not_null @description(#\"\r\n    The answer to the user's question or request.\r\n  \"#)\r\n  @@dynamic\r\n}\r\n\r\nfunction Betty(instruction: string, messages: Message[], relevant_feedbacks: string, user_metadata: UserMetadata, user_memory: string) -> ResponseChat {\r\n  // Specify a client as provider/model-name\r\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\r\n  client CustomGPT41 // Set OPENAI_API_KEY to use this client.\r\n  prompt #\"\r\n\r\n    {{ BettyCorePrompt() }}\r\n\r\n    {{ instruction }}\r\n\r\n    {{ RelevantFeedbacks(relevant_feedbacks) }}\r\n\r\n    {{ UserMetadataTemplate(user_metadata) }}\r\n\r\n    {{ UserMemory(user_memory) }}\r\n\r\n    {{ ConversationHistory(messages) }}\r\n\r\n    {{ _.role(\"system\") }}\r\n\r\n    Provide your response.\r\n\r\n    {{ ctx.output_format }}\r\n  \"#\r\n}\r\nfunction InitialMessageChat(instruction: string, relevant_feedbacks: string, user_influence_style: string, user_memory: string) -> string {\r\n  client CustomGPT41\r\n  prompt #\"\r\n    Generate an initial message for the user to start the chat. No need to introduce the app as if it is their first time.\r\n\r\n    {{ BettyCorePrompt() }}\r\n\r\n    {{ instruction }}\r\n\r\n    {{ RelevantFeedbacks(relevant_feedbacks) }}\r\n\r\n    {{ UserInfluenceStyle(user_influence_style) }}\r\n\r\n    {{ UserMemory(user_memory) }}\r\n\r\n    Answer:\r\n  \"#\r\n}\r\ntest TestBetty {\r\n    functions [Betty]\r\n    type_builder {\r\n        dynamic class ResponseChat {\r\n            tool_calls (SwitchInfluenceStyle)[]\r\n        }\r\n    }\r\n    args {\r\n        instruction \"You are Betty, an assistant coaching real estate business to the users.\"\r\n        messages [\r\n            {role \"user\", content \"Hi, nice to meet you.\"},\r\n            {role \"assistant\", content \"Hi, nice to meet you too.\"},\r\n            {role \"user\", content \"What is my influence style?\"},\r\n            {role \"assistant\", content \"You are a diplomat.\"},\r\n            {role \"user\", content \"No, I am Catalyst, change my influence style to Catalyst.\"},\r\n        ]\r\n        relevant_feedbacks \"Behave like a diplomat. You should be friendly and engaging.\"\r\n        user_influence_style \"Diplomat\"\r\n        user_memory \"The user is a real estate business owner.\"\r\n    }\r\n}\r\n\r\ntest TestInitialMessageChat {\r\n    functions [InitialMessageChat]\r\n    args {\r\n        instruction \"You are Betty, an assistant coaching real estate business to the users.\"\r\n        relevant_feedbacks \"Behave like a diplomat. You should be friendly and engaging.\"\r\n        user_influence_style \"Catalyst\"\r\n        user_memory \"The user is a real estate business owner.\"\r\n    }\r\n}",
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\r\n\r\nclient<llm> CustomGPT41 {\r\n  provider openai\r\n  options {\r\n    model \"gpt-4.1\"\r\n    api_key env.OPENAI_API_KEY\r\n  }\r\n}\r\n\r\nclient<llm> CustomGPT4o {\r\n  provider openai\r\n  options {\r\n    model \"gpt-4o\"\r\n    api_key env.OPENAI_API_KEY\r\n  }\r\n}\r\n\r\nclient<llm> CustomGPT4oMini {\r\n  provider openai\r\n  retry_policy Exponential\r\n  options {\r\n    model \"gpt-4o-mini\"\r\n    api_key env.OPENAI_API_KEY\r\n  }\r\n}\r\n\r\nclient<llm> CustomSonnet {\r\n  provider anthropic\r\n  options {\r\n    model \"claude-3-5-sonnet-20241022\"\r\n    api_key env.ANTHROPIC_API_KEY\r\n  }\r\n}\r\n\r\n\r\nclient<llm> CustomHaiku {\r\n  provider anthropic\r\n  retry_policy Constant\r\n  options {\r\n    model \"claude-3-haiku-20240307\"\r\n    api_key env.ANTHROPIC_API_KEY\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\r\nclient<llm> CustomFast {\r\n  provider round-robin\r\n  options {\r\n    // This will alternate between the two clients\r\n    strategy [CustomGPT4oMini, CustomHaiku]\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\r\nclient<llm> OpenaiFallback {\r\n  provider fallback\r\n  options {\r\n    // This will try the clients in order until one succeeds\r\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/retry\r\nretry_policy Constant {\r\n  max_retries 3\r\n  // Strategy is optional\r\n  strategy {\r\n    type constant_delay\r\n    delay_ms 200\r\n  }\r\n}\r\n\r\nretry_policy Exponential {\r\n  max_retries 2\r\n  // Strategy is optional\r\n  strategy {\r\n    type exponential_backoff\r\n    delay_ms 300\r\n    multiplier 1.5\r\n    max_delay_ms 10000\r\n  }\r\n}",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\r\n// your choice. You can have multiple generators if you use multiple languages.\r\n// Just ensure that the output_dir is different for each generator.\r\ngenerator typescript {\r\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\r\n    output_type \"typescript/react\"\r\n\r\n    // Where the generated code will be saved (relative to baml_src/)\r\n    output_dir \"../app/\"\r\n\r\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\r\n    // The BAML VSCode extension version should also match this version.\r\n    version \"0.88.0\"\r\n\r\n    // Valid values: \"sync\", \"async\"\r\n    // This controls what `b.FunctionName()` will be (sync or async).\r\n    default_client_mode async\r\n}\r\n",
  "tools.baml": "class SwitchInfluenceStyle {\r\n    name \"switch_influence_style\" @description(#\"\r\n        The name of the tool.\r\n    \"#)\r\n    influence_style string @description(#\"\r\n        The influence style to switch to.\r\n    \"#)\r\n}",
}
export const getBamlFiles = () => {
    return fileMap;
}